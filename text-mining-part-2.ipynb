{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.randint(10,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time as t\n",
    "from lxml import html \n",
    "import requests\n",
    "import random\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15'\n",
    "headers = {'User-Agent': user_agent}\n",
    "urls = []\n",
    "searchlink = 'https://www.yelp.com/search?find_desc=&find_loc=Clemson%2C+SC&ns=1'\n",
    "\n",
    "\n",
    "# get page counts\n",
    "page = requests.get(searchlink, headers = headers)\n",
    "parser = html.fromstring(page.content)\n",
    "page_nums = '//span[@class=\"lemon--span__373c0__3997G text__373c0__2pB8f text-color--normal__373c0__K_MKN text-align--left__373c0__2pnx_\"]'\n",
    "pg = parser.xpath(page_nums)\n",
    "\n",
    "page_count = int(pg[0].text.split()[3])\n",
    "suffix = \"\"\n",
    "for i in range(1,(page_count + 1)):\n",
    "    searchlink = 'https://www.yelp.com/search?find_desc=&find_loc=Clemson%2C+SC&ns=1' + suffix\n",
    "    print(searchlink)\n",
    "    page = requests.get(searchlink, headers = headers)\n",
    "    parser = html.fromstring(page.content)\n",
    "    businesslink=parser.xpath('//a[@class=\"lemon--a__373c0__IEZFH link__373c0__29943 link-color--blue-dark__373c0__1mhJo link-size--inherit__373c0__2JXk5\"]')\n",
    "    links = [l.get('href') for l in businesslink]\n",
    "    true_links = [link for link in links if link.startswith('/biz') and not \"hrid\" in link]\n",
    "    for link in true_links:\n",
    "        url = 'https://www.yelp.com' + str(link)\n",
    "        urls.append(url)\n",
    "    t.sleep(random.randint(8,15))\n",
    "    suffix = '&start=' + str(10 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the number of URLs generated are the same as the number of restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item in urls:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook for part 1, we still have not worked on textual analysis yet. We have not even mined the text yet. What we have done was to mine the URLs pointing to individual restaurants. These URLs are stored in a list, which is represented by variable **urls**. \n",
    "\n",
    "In this notebook, first we will download the reviews from the individual restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patterns, patterns, and patterns\n",
    "\n",
    "The principles are the same for the reviews. We want to go to the restaurant's page, identify the HTML tag/class containing the reviews, extract the components of the reviews (text, rating, date, ...), and finally, repeat the entire process across all the review pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the begining, we go through the same process at Part 1 to set up, with things like user-agent information and URL to restaurant's page. In this scenario, we will use the first item in the restaurant URL list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import time as t\n",
    "from lxml import html \n",
    "from lxml import etree\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.1.2 Safari/605.1.15'\n",
    "headers = {'User-Agent': user_agent}\n",
    "searchlink = 'https://www.yelp.com/biz/yolk-asian-kitchen-clemson-3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scape the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(searchlink, headers = headers)\n",
    "parser = html.fromstring(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the reviews\n",
    "\n",
    "We want to get everything. In this case, it means that extracting a large blob of HTML codes, then go through the smaller tags individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=parser.xpath('//div[@class=\"review review--with-sidebar\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for review in reviews:\n",
    "    temp = review.xpath('.//div[contains(@class, \"biz-rating__stars\")]/div')\n",
    "    rating = [td.get('title') for td in temp]\n",
    "       \n",
    "    xpath_author  = './/a[@id=\"dropdown_user-name\"]//text()'\n",
    "    xpath_body    = './/p[@lang=\"en\"]//text()'\n",
    "    \n",
    "    author  = review.xpath(xpath_author)\n",
    "    date    = review.xpath('.//span[@class=\"rating-qualifier\"]//text()')\n",
    "    body    = review.xpath(xpath_body)\n",
    "        \n",
    "    # we are extracting the business title from the main page. \n",
    "    heading= parser.xpath('//h1[contains(@class,\"biz-page-title embossed-text-white\")]')\n",
    "    bzheading = [td.text for td in heading]\n",
    "    print (author, rating, date, body, bzheading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the dataframe created at the begining of the part-1 notebook, and we never used it. We are using it now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.DataFrame()\n",
    "\n",
    "for review in reviews:\n",
    "    temp = review.xpath('.//div[contains(@class, \"biz-rating__stars\")]/div')\n",
    "    rating = [td.get('title') for td in temp]\n",
    "       \n",
    "    xpath_author  = './/a[@id=\"dropdown_user-name\"]//text()'\n",
    "    xpath_body    = './/p[@lang=\"en\"]//text()'\n",
    "    \n",
    "    author  = review.xpath(xpath_author)\n",
    "    date    = review.xpath('.//span[@class=\"rating-qualifier\"]//text()')\n",
    "    body    = review.xpath(xpath_body)\n",
    "        \n",
    "    # we are extracting the business title from the main page. \n",
    "    heading= parser.xpath('//h1[contains(@class,\"biz-page-title embossed-text-white\")]')\n",
    "    bzheading = [td.text for td in heading]\n",
    "    \n",
    "    # instead of printing out the values, we now create a dictionary, and append it \n",
    "    # to the data frame\n",
    "    review_dict = {'restaurant' : bzheading,\n",
    "                'rating': rating, \n",
    "                'author': author, \n",
    "                'date': date,\n",
    "                'Review': body,\n",
    "              }\n",
    "    reviews_df = reviews_df.append(review_dict, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the populated dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous notebook, now that we get all the reviews on one page, we need to get the reviews on all pages. \n",
    "\n",
    "Find the patterns!\n",
    "\n",
    "https://www.yelp.com/biz/yolk-asian-kitchen-clemson-3\n",
    "\n",
    "https://www.yelp.com/biz/yolk-asian-kitchen-clemson-3?start=20\n",
    "\n",
    "https://www.yelp.com/biz/yolk-asian-kitchen-clemson-3?start=40\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get page counts\n",
    "page = requests.get(searchlink, headers = headers)\n",
    "parser = html.fromstring(page.content)\n",
    "page_nums = '//div[@class=\"page-of-pages arrange_unit arrange_unit--fill\"]'\n",
    "pg = parser.xpath(page_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pg[0].text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_count = int(pg[0].text.split()[3])\n",
    "suffix = \"\"\n",
    "\n",
    "# setup empty data frame for reviews\n",
    "reviews_df = pd.DataFrame()\n",
    "# loop through all review pages\n",
    "for i in range(1,(page_count + 1)):\n",
    "    searchlink = 'https://www.yelp.com/biz/yolk-asian-kitchen-clemson-3' + suffix\n",
    "    print(searchlink)\n",
    "    page = requests.get(searchlink, headers = headers)\n",
    "    parser = html.fromstring(page.content)\n",
    "    reviews=parser.xpath('//div[@class=\"review review--with-sidebar\"]')    \n",
    "\n",
    "    for review in reviews:\n",
    "        temp = review.xpath('.//div[contains(@class, \"biz-rating__stars\")]/div')\n",
    "        rating = [td.get('title') for td in temp]\n",
    "       \n",
    "        xpath_author  = './/a[@id=\"dropdown_user-name\"]//text()'\n",
    "        xpath_body    = './/p[@lang=\"en\"]//text()'\n",
    "    \n",
    "        author  = review.xpath(xpath_author)\n",
    "        date    = review.xpath('.//span[@class=\"rating-qualifier\"]//text()')\n",
    "        body    = review.xpath(xpath_body)\n",
    "        \n",
    "        # we are extracting the business title from the main page. \n",
    "        heading= parser.xpath('//h1[contains(@class,\"biz-page-title embossed-text-white\")]')\n",
    "        bzheading = [td.text for td in heading]\n",
    "    \n",
    "        # instead of printing out the values, we now create a dictionary, and append it \n",
    "        # to the data frame\n",
    "        review_dict = {'restaurant' : bzheading,\n",
    "                    'rating': rating, \n",
    "                    'author': author, \n",
    "                    'date': date,\n",
    "                    'Review': body,\n",
    "                    }\n",
    "        reviews_df = reviews_df.append(review_dict, ignore_index=True)\n",
    "    t.sleep(5)\n",
    "    suffix = '?start=' + str(20 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it for ALL RESTAURANTS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an additional outer loop to go through all the restaurants ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we move the initial empty data frame outside, because\n",
    "# we want all reviews from all restaurants to be stored in the same data frame. \n",
    "reviews_df = pd.DataFrame()\n",
    "\n",
    "# loop through all restaurants\n",
    "for url in urls:\n",
    "    page = requests.get(url, headers = headers)\n",
    "    parser = html.fromstring(page.content)\n",
    "    page_nums = '//div[@class=\"page-of-pages arrange_unit arrange_unit--fill\"]'\n",
    "    pg = parser.xpath(page_nums)\n",
    "    page_count = int(pg[0].text.split()[3])\n",
    "    suffix = \"\"\n",
    "    print(\"Restaurant\",url,\"has\",page_count,\"review pages\")\n",
    "\n",
    "    # loop through all review pages\n",
    "    for i in range(1,(page_count + 1)):\n",
    "        searchlink = url + suffix\n",
    "        page = requests.get(searchlink, headers = headers)\n",
    "        parser = html.fromstring(page.content)\n",
    "        reviews=parser.xpath('//div[@class=\"review review--with-sidebar\"]')    \n",
    "\n",
    "        for review in reviews:\n",
    "            temp = review.xpath('.//div[contains(@class, \"biz-rating__stars\")]/div')\n",
    "            rating = [td.get('title') for td in temp]\n",
    "       \n",
    "            xpath_author  = './/a[@id=\"dropdown_user-name\"]//text()'\n",
    "            xpath_body    = './/p[@lang=\"en\"]//text()'\n",
    "    \n",
    "            author  = review.xpath(xpath_author)\n",
    "            date    = review.xpath('.//span[@class=\"rating-qualifier\"]//text()')\n",
    "            body    = review.xpath(xpath_body)\n",
    "        \n",
    "            # we are extracting the business title from the main page. \n",
    "            heading= parser.xpath('//h1[contains(@class,\"biz-page-title embossed-text-white\")]')\n",
    "            bzheading = [td.text for td in heading]\n",
    "    \n",
    "            # instead of printing out the values, we now create a dictionary, and append it \n",
    "            # to the data frame\n",
    "            review_dict = {'restaurant' : bzheading,\n",
    "                        'rating': rating, \n",
    "                        'author': author, \n",
    "                        'date': date,\n",
    "                        'Review': body,\n",
    "                        }\n",
    "            reviews_df = reviews_df.append(review_dict, ignore_index=True)\n",
    "        t.sleep(random.randint(10,15))\n",
    "        suffix = '?start=' + str(20 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv(\"yelp_reviews_clemson_sc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some cleaning on our data ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary symbols and phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_reviews_df['Review'] = reviews_df.Review.str[2:-2]\n",
    "clean_reviews_df['author'] = reviews_df.author.str[2:-2]\n",
    "clean_reviews_df['date'] = reviews_df.date.str[12:-8]\n",
    "clean_reviews_df['rating'] = reviews_df.rating.str[2:-2]\n",
    "clean_reviews_df['restaurant'] = reviews_df.restaurant.str[16:-12]\n",
    "clean_reviews_df['rating'] = reviews_df.rating.str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv(\"yelp_reviews_clemson_sc.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "Perform the extraction of all review data from all restaurants in your home town and save it to a csv file. \n",
    "\n",
    "Provide answers to the following questions based on your collected data:\n",
    "\n",
    "- Which restaurant has the highest average rating?\n",
    "- Which restaurant has the lowest average rating?\n",
    "- What is the average rating of your favorite restaurant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
